{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] O sistema nÃ£o pode encontrar o arquivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set up a virtual display to render the Lunar Lander environment.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m840\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set the random seed for TensorFlow\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(utils\u001b[38;5;241m.\u001b[39mSEED)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyvirtualdisplay\\display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyvirtualdisplay\\xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[1;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyvirtualdisplay\\abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyvirtualdisplay\\util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema nÃ£o pode encontrar o arquivo especificado"
     ]
    }
   ],
   "source": [
    "# Set up a virtual display to render the Lunar Lander environment.\n",
    "Display(visible=0, size=(840, 480)).start()\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lunar Lander Environment\n",
    "\n",
    "In this notebook we will be using [OpenAI's Gym Library](https://www.gymlibrary.dev/). The Gym library provides a wide variety of environments for reinforcement learning. To put it simply, an environment represents a problem or task to be solved. In this notebook, we will try to solve the Lunar Lander environment using reinforcement learning.\n",
    "\n",
    "The goal of the Lunar Lander environment is to land the lunar lander safely on the landing pad on the surface of the moon. The landing pad is designated by two flag poles and its center is at coordinates `(0,0)` but the lander is also allowed to land outside of the landing pad. The lander starts at the top center of the environment with a random initial force applied to its center of mass and has infinite fuel. The environment is considered solved if you get `200` points. \n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Fire right engine.\n",
    "* Fire main engine.\n",
    "* Fire left engine.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The agent's observation space consists of a state vector with 8 variables:\n",
    "\n",
    "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
    "* Its linear velocities $(\\dot x,\\dot y)$.\n",
    "* Its angle $\\theta$.\n",
    "* Its angular velocity $\\dot \\theta$.\n",
    "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
    "\n",
    "* The absolute value of the lander's $x$-coordinate is greater than 1 (i.e. it goes beyond the left or right border)\n",
    "\n",
    "You can check out the [Open AI Gym documentation](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) for a full description of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the `LunarLander-v2` environment from the `gym` library by using the `.make()` method. `LunarLander-v2` is the latest version of the Lunar Lander environment and you can read about its version history in the [Open AI Gym documentation](https://www.gymlibrary.dev/environments/box2d/lunar_lander/#version-history).\n",
    "\n",
    "Once we load the environment we use the `.reset()` method to reset the environment to the initial state. The lander starts at the top center of the environment and we can render the first frame of the environment by using the `.render()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If error, run: \n",
    "# pip install swig\n",
    "# pip install gymnasium[box2d]\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "image = env.render()\n",
    "\n",
    "PIL.Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our neural network later on we need to know the size of the state vector and the number of valid actions. We can get this information from our environment by using the `.observation_space.shape` and `action_space.n` methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the standard âagent-environment loopâ formalism, an agent interacts with the environment in discrete time steps $t=0,1,2,...$. At each time step $t$, the agent uses a policy $\\pi$ to select an action $A_t$ based on its observation of the environment's state $S_t$. The agent receives a numerical reward $R_t$ and on the next time step, moves to a new state $S_{t+1}$.\n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Exploring the Environment's Dynamics\n",
    "\n",
    "In Open AI's Gym environments, we use the `.step()` method to run a single time step of the environment's dynamics. In the version of `gym` that we are using the `.step()` method accepts an action and returns four values:\n",
    "\n",
    "* `observation` (**object**): an environment-specific object representing your observation of the environment. In the Lunar Lander environment this corresponds to a numpy array containing the positions and velocities of the lander as described in section [3.2 Observation Space](#3.2).\n",
    "\n",
    "\n",
    "* `reward` (**float**): amount of reward returned as a result of taking the given action. In the Lunar Lander environment this corresponds to a float of type `numpy.float64` as described in section [3.3 Rewards](#3.3).\n",
    "\n",
    "\n",
    "* `done` (**boolean**): When done is `True`, it indicates the episode has terminated and itâs time to reset the environment. \n",
    "\n",
    "\n",
    "* `info` (**dictionary**): diagnostic information useful for debugging. We won't be using this variable in this notebook but it is shown here for completeness.\n",
    "\n",
    "To begin an episode, we need to reset the environment to an initial state. We do this by using the `.reset()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "current_state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'display_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display table with values.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_table\u001b[49m(current_state, action, next_state, reward, done)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Replace the `current_state` with the state after the action is taken\u001b[39;00m\n\u001b[0;32m     11\u001b[0m current_state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'display_table'"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Display table with values.\n",
    "utils.display_table(current_state, action, next_state, reward, done)\n",
    "\n",
    "# Replace the `current_state` with the state after the action is taken\n",
    "current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
    "$$\n",
    "\n",
    "This iterative method converges to the optimal action-value function $Q^*(s,a)$ as $i\\to\\infty$. This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of $Q(s,a)$ until it converges to the optimal action-value function $Q^*(s,a)$. However, in cases where the state space is continuous it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate $Q(s,a)$ until it converges to $Q^*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Deep $Q$-Learning, we solve this problem by using a neural network to estimate the action-value function $Q(s,a)\\approx Q^*(s,a)$. We call this neural network a $Q$-Network and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.\n",
    "\n",
    "Unfortunately, using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. To avoid instabilities we can implement ***Target Network*** and ***Experience Replay***. We will explore these two techniques in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create\n",
    "a separate neural network for generating the $y$ targets. We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    " \n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(shape=(state_size)),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=(state_size)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "        experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "        gamma: (float) The discount factor.\n",
    "        q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "        target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "    \n",
    "    Returns:\n",
    "        loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values) \n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
