{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms are a subset of unsupervised machine learning techniques that group similar data points together based on certain characteristics or features. The main goal of clustering is to partition a dataset into groups, or clusters, where the data points within each cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "There are various clustering algorithms, each with its own approach and characteristics. Some of the most common clustering algorithms include K-means, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian Mixture Models (GMM).\n",
    "\n",
    "K-means is one of the simplest and most widely used clustering algorithms. It partitions the dataset into K clusters by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the mean of the data points assigned to each cluster.\n",
    "\n",
    "Hierarchical clustering builds a tree-like hierarchy of clusters by either starting with individual data points as clusters and then merging them iteratively, or by starting with all data points as one cluster and then splitting them recursively.\n",
    "\n",
    "DBSCAN identifies clusters based on the density of data points in the feature space. It groups together closely packed points and marks points in low-density regions as outliers.\n",
    "\n",
    "Gaussian Mixture Models represent the distribution of data points as a mixture of multiple Gaussian distributions. It models each cluster as a Gaussian distribution and assigns data points probabilistically to each cluster.\n",
    "\n",
    "Choosing the appropriate clustering algorithm depends on factors such as the structure of the data, the desired number of clusters, and the computational resources available. Evaluating clustering results often involves metrics such as silhouette score, Daviesâ€“Bouldin index, or visual inspection. Clustering finds applications in various fields including pattern recognition, image analysis, customer segmentation, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* K-means is an iterative procedure that\n",
    "     * Starts by guessing the initial centroids, and then \n",
    "     * Refines this guess by \n",
    "         * Repeatedly assigning examples to their closest centroids, and then \n",
    "         * Recomputing the centroids based on the assignments.\n",
    "\n",
    "* The inner-loop of the algorithm repeatedly carries out two steps: \n",
    "    1. Assigning each training example $x^{(i)}$ to its closest centroid, and\n",
    "    2. Recomputing the mean of each centroid using the points assigned to it. \n",
    "    \n",
    "    \n",
    "* The $K$-means algorithm will always converge to some final set of means for the centroids. \n",
    "\n",
    "* However, the converged solution may not always be ideal and depends on the initial setting of the centroids.\n",
    "    * Therefore, in practice the K-means algorithm is usually run a few times with different random initializations. \n",
    "    * One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    K = centroids.shape[0]\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        distance = [] \n",
    "        for j in range(centroids.shape[0]):\n",
    "            # Compute Euclidean distance\n",
    "            norm_ij = np.linalg.norm(X[i] - centroids[j])\n",
    "            distance.append(norm_ij)\n",
    "        idx[i] = np.argmin(distance)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every centroid $\\mu_k$, is seted\n",
    "\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$ \n",
    "\n",
    "    where \n",
    "    * $C_k$ is the set of examples that are assigned to centroid $k$\n",
    "    * $|C_k|$ is the number of examples in the set $C_k$\n",
    "\n",
    "\n",
    "If two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k=2$,\n",
    "then you should update $\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = X.shape\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    for k in range(K):\n",
    "        X_k = X[idx == k]\n",
    "        c_k = np.mean(X_k, axis=0)\n",
    "        \n",
    "        # Alternative:\n",
    "        # X_k= []\n",
    "        # for i, id in enumerate(idx):\n",
    "        #     if id == k:\n",
    "        #         X_k2.append(X[i])\n",
    "\n",
    "        # c_k = sum(X_k2) / len(X_k2)\n",
    "        \n",
    "        centroids[k] = c_k\n",
    "    \n",
    "    return centroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
