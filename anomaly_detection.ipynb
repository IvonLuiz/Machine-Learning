{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use anomaly detection vs. when to use supervised learning:\n",
    "\n",
    "- **Anomaly detection:**\n",
    "    - Very few positive examples (y = 1) (0-20 is common), with a large number of negative examples (y = 0).\n",
    "    - If there are many different \"types\" of anomalies. It's challenging for any algorithm to learn from positive examples what the anomalies look like; future anomalies may bear no resemblance to any of the anomalous examples encountered thus far.\n",
    "    - Examples: \n",
    "        - Fraud detection.\n",
    "        - Manufacturing: finding new previously unseen defects in manufacturing.\n",
    "        - Monitoring machines in a data center (different types of attacks or malfunctions).\n",
    "\n",
    "- **Supervised learning:**\n",
    "    - Large number of positive and negative examples.\n",
    "    - Sufficient positive examples for the algorithm to grasp what positive examples entail; future positive examples are likely to resemble those in the training set.\n",
    "    - Examples: \n",
    "        - Email spam classification.\n",
    "        - Manufacturing: finding known previously seen defects.\n",
    "        - Weather prediction (sunny/rainy/etc.).\n",
    "        - Diseases classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform anomaly detection, you will first need to fit a model to the data’s distribution.\n",
    "\n",
    "* Given a training set $\\{x^{(1)}, ..., x^{(m)}\\}$ you want to estimate the Gaussian distribution for each of the features $x_i$. \n",
    "\n",
    "* Recall that the Gaussian distribution is given by\n",
    "\n",
    "   $$ p(x ; \\mu,\\sigma ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x - \\mu)^2}{2 \\sigma ^2} }$$\n",
    "\n",
    "   where $\\mu$ is the mean and $\\sigma^2$ is the variance.\n",
    "   \n",
    "* For each feature $i = 1\\ldots n$, you need to find parameters $\\mu_i$ and $\\sigma_i^2$ that fit the data in the $i$-th dimension $\\{x_i^{(1)}, ..., x_i^{(m)}\\}$ (the $i$-th dimension of each example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the parameters, ($\\mu_i$, $\\sigma_i^2$), of the $i$-th feature by using the following equations. To estimate the mean, you will use:\n",
    "\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "\n",
    "and for the variance you will use:\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    mu = np.sum(X, axis=0) / m\n",
    "    var = np.sum((X - mu) ** 2, axis=0) / m\n",
    "    \n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian(X, mu, var):\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement code to calculate the F1 score from choosing `epsilon` as the threshold and place the value in `F1`. \n",
    "\n",
    "  * Recall that if an example $x$ has a low probability $p(x) < \\varepsilon$, then it is classified as an anomaly. \n",
    "        \n",
    "  * Then, you can compute precision and recall by: \n",
    "      \n",
    "  $$\\begin{aligned}\n",
    "  prec&=&\\frac{tp}{tp+fp}\\\\\n",
    "  rec&=&\\frac{tp}{tp+fn},\n",
    "  \\end{aligned}$$ \n",
    "    \n",
    "    * where:\n",
    "        * $tp$ is the number of true positives: the ground truth label says it’s an anomaly and our algorithm correctly classified it as an anomaly.\n",
    "        * $fp$ is the number of false positives: the ground truth label says it’s not an anomaly, but our algorithm incorrectly classified it as an anomaly.\n",
    "        * $fn$ is the number of false negatives: the ground truth label says it’s an anomaly, but our algorithm incorrectly classified it as not being anomalous.\n",
    "\n",
    "  * The $F_1$ score is computed using precision ($prec$) and recall ($rec$) as follows:\n",
    "    $$F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "    \n",
    "        predictions = (p_val < epsilon)\n",
    "        \n",
    "        tp = sum((predictions == 1) & (y_val == 1))\n",
    "        fp = sum((predictions == 1) & (y_val == 0))\n",
    "        tn = sum((predictions == 0) & (y_val == 0))\n",
    "        fn = sum((predictions == 0) & (y_val == 1))\n",
    "        \n",
    "        prec = tp / (tp + fp)        \n",
    "        rec = tp / (tp + fn)\n",
    "        \n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
